---
marp: true
---

# Bachelor's Thesis Ideas
### for robotic tasks with sparse-reward RL 
Joong-Won Seo

---

# Potential Areas of Research

Extend on:
1) HER & HGG (tackle sparse-reward RL)
2) G-HGG && BBox-HGG (tackle obstacle avoidance / navigation)
3) I-HGG & MultiCam-HGG (tackle environment perception)

* Focus on **(2) Navigation** & **(3) Perception**

---

# Limitations in Navigation

Trained in static/deterministic environments environments (even moving obstacles have deterministic movement)
* Cannot react to new environments not seen in training
* Cannot handle unknown moving obstacles with complex movement (e.g. humans)

---

# Some Solution Ideas

1) Temporal info with RNN could help with dynamic environments & navigation?
    World Models with `Perception -> Next state prediction -> Controller` loop can:
    * learn the dynamics from visual input in unsupervised manner (*world model*)
    * learn to navigate or avoid obstacles
    * can even train the controller only in its "dream" generated by the world model
2) Path Planning via RL / Shortest Path Approximation seems to exist
    * maybe integrating it into the NN can help it find paths dynamically?
    * or explicitly use the generated path as intermediate goals in a multi-step goal, e. g. splitting the path into straight movements 

* Generally seems hard...

---

# Limitations in Perception

Trained with static camera configuration (amount, position, orientation, resolution, etc.)
* Cannot adapt to new camera configurations
    -> requires retraining or even architecture adjustment after change
* Camera is not allowed to move dynamically
    -> problematic for big environments or highly occluded areas <!--if something is hidden, we can just move so that we can see it-->

---

# Solution Idea

Working with 3D input instead of 2D images:
* LIDAR or Depth-sensing cameras like Kinect, returning a point cloud
* Even 2D camera setups could be used after using depth estimation algorithms
* Sensor configuration does not matter:
    * if the position/orientation of the sensor is known, easy to transform the point cloud into absolute coordinates
    * trivial to combine any number of sensors (union of all absolute coordinates)
    * easy to adjust the number of points for performance scaling
* Can be converted to/from mesh/voxel/depth map if required
![img w:450](img/3d_rep.png)

---

# Simplest Approach with Voxel-CNN

Just extend the idea of 2D CNNs to 3D CNN...?

**PROBLEMS**:
* Curse of Dimensionality ($128^3 \approx 2,000,000$) -> too low-res for segmentation
* fixed resolution
* very sparse points -> inefficient data usage

---

# Deep Learning on Point Clouds with PointNet

Working directly with raw point clouds

* Unordered set of points: utilizes *input permutation invariance*!
* Also robust against geometric transformations and data corruption <!--i.e. missing data, outlier points, perturbation noise-->
* Achieves state-of-the-art feature learning tasks: ![w:500](https://stanford.edu/~rqi/pointnet/images/teaser.jpg)
* Much more efficient than naive 3D CNNs
* Very high-res, since raw coordinate data is input
* Any number of points without architecture change/retraining

---

# Derivations/Neighbors of PointNet

Things based on or similar to PointNet:
* Autoencoder:
![w:400](https://github.com/charlesq34/pointnet-autoencoder/raw/master/doc/teaser.jpg)
* PointNet++: Recursive Hierarchial Feature Learning
* VoxelNet: Region proposal, 3D bounding box estimation
* PointPillars: Much faster speed for real-time usage

---

# Foreseeable Challenges

* Segmentation in a self-supervised/unsupervised manner?
* How to determine unexpected/unidentifiable objects never seen in training? (Unknown Object Detection)
* Goal state representation?
* ...

---



---

# Random Interests

- Vision Transformers:
    - seem very promising, outperforms CNNs often!
    - hard training, requiring more training data, but result is often more efficient and better
        -> pre-trained models available!

