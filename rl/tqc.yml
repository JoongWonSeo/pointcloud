# add this into rl-baselines3-zoo/hyperparams/tqc.yml


# === My Custom Robotics GoalEnvs ===
RoboReach-v0:
  n_timesteps: !!float 40000
  policy: 'MultiInputPolicy'
  buffer_size: 1000000
  ent_coef: 'auto'
  batch_size: 256
  gamma: 0.95
  learning_rate: 0.001
  learning_starts: 1000
  normalize: False
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"
  policy_kwargs: "dict(net_arch=[64, 64], n_critics=1)"
  n_envs: 4
  gradient_steps: -1

VisionReach-v0:
  n_timesteps: !!float 40000
  policy: 'MultiInputPolicy'
  buffer_size: 1000000
  ent_coef: 'auto'
  batch_size: 256
  gamma: 0.95
  learning_rate: 0.001
  learning_starts: 1000
  normalize: False
  replay_buffer_class: HerReplayBuffer
  replay_buffer_kwargs: "dict(
    goal_selection_strategy='future',
    n_sampled_goal=4
  )"
  policy_kwargs: "dict(net_arch=[64, 64], n_critics=1)"
  n_envs: 2
  gradient_steps: -1

RoboPush-v0:
  <<: *her-defaults
  learning_starts: 1000
  n_envs: 4
  gradient_steps: -1

RoboPickAndPlace-v0:
  <<: *her-defaults
  learning_starts: 1000
  n_envs: 4
  gradient_steps: -1

VisionPushMultiSeg-v0:
  <<: *her-defaults
  learning_starts: 1000
  n_envs: 4
  gradient_steps: -1